import random  # for choosing the random responses
import json
import pickle # for sterilization
import numpy as np
import tensorflow as tf

import nltk  # natural language toolkit, a primary tool for performing Natural Language Processing (NLP) tasks
from nltk.stem import WordNetLemmatizer # this reduces the words to their stems, so we don't lose performances because the bot looks for the exact words

from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Activation, Dropout
from tensorflow.keras.optimizers import SGD


# here we will be lemmatizing the entire document (intents)
lemmatizer = WordNetLemmatizer()

intents = json.loads(open('intents.json').read()) # reads the content of the json file as text, then pass the text to the load function

# creating 3 empty lists and specify the ignore letters (letters that will not be accounted for)
words = []
classes = []
documents = []
ignore_letters = ['?', '!', '.', ','] 

# now iterate over the intent
for intent in intents['intents']: # we need to access the key intents in the dictionary intent which is the object. for each intent, we have subvalues
    for pattern in intent['patterns']:
        word_list = nltk.word_tokenize(pattern) # tokenization here splits a sentence into individual words
        words.extend(word_list) # extend here is taking the content and appending it to the list
        documents.append((word_list, intent['tag'])) # appending a tuple of the word list and of the classes of the particular intent
        if intent['tag'] not in classes:
            classes.append(intent['tag'])
    
words = [lemmatizer.lemmatize(word) for word in words if word not in ignore_letters]
words = sorted(set(words))

classes = sorted(set(classes))

pickle.dump(words, open('words.pkl', 'wb')) # saving them into pickle files as writting binaries
pickle.dump(classes, open('classes.pkl', 'wb'))


training = []  # creating an empty list for training
output_empty = [0] * len(words) # creating an output which is a template of zeros, we need as many zeros as the classes

# preprocessing
for document in documents:
    bag = []
    word_patterns = document[0]
    word_patterns = [lemmatizer.lemmatize(word.lower()) for word in word_patterns]
    for word in words:
        bag.append(1) if word in word_patterns else bag.append(0)

    output_row = list(output_empty)
    output_row[classes.index(document[1])] = 1
    training.append([bag, output_row])
    
random.shuffle(training)
training = np.array(training)


train_x = list(training[:, 0])
train_y = list(training[:, 1])


model = Sequential()
model.add(Dense(128, input_shape=(len(train_x[0]),), activation = 'relu'))
model.add(Dropout(0.5))
model.add(Dense(64, activation = 'relu'))
model.add(Dropout(0.5))
model.add(Dense(len(train_y[0]), activation='softmax'))

sgd = SGD(learning_rate=0.01, decay = 1e-6, momentum=0.9, nesterov=True)
model.compile(loss='categorical_crossentropy', optimizer=sgd, metrics=['accuracy'])

hist = model.fit(np.array(train_x), np.array(train_y), epochs=200, batch_size=5, verbose=1)
model.save('chatbotmodel.keras', hist)
print('Done')
